{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi-Layer Perceptron (MLP) - Large & Complex Data Set\n",
    "\n",
    "\n",
    "In this notebook we will train MLP classifier on a **large & non-linear data set**. We will use an image classification (multi-class) problem for experimentation. \n",
    "\n",
    "For an analytical understanding, we will compare the performance of the MLP with the SVM and Logistic Regression classifiers.\n",
    "\n",
    "\n",
    "Due to the non-linearity of the features (i.e., pixels), we will use the Gaussian Radial Basis Function (RBF) Kernel based Support Vector Machine (SVM). Previously we have seen that the Gaussian RBF Kernel based SVM performs better than Softmax regression classifier.\n",
    "\n",
    "To expedite the training time, we use dimensionality reduction technique (Principle Component Analysis) to project the features into a smaller dimension.\n",
    "\n",
    "Our goal is to investigate whether MLP outperforms the Gaussian RBF Kernel SVM on a very large complex data set.\n",
    "\n",
    "We conduct the following experiments.\n",
    "\n",
    "\n",
    "## Experiments\n",
    "\n",
    "- Experiment 1: Multi-Layer Perceptron \n",
    "- Experiment 2: Multi-Layer Perceptron + PCA\n",
    "- Experiment 2: Support Vector Machine (SVC with RBF Kernel) + PCA\n",
    "- Experiment 3: Logistic Regression (Softmax Regression) + PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST\n",
    "\n",
    "\n",
    "We will use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixelâ€™s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)\n",
    "\n",
    "First load the data and explore the feature names, target names, etc.\n",
    "\n",
    "We may load the data from a local folder or load it directly from cloud using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Samples:  (70000, 784)\n",
      "No. of Labels:  (70000,)\n",
      "\n",
      "X type:  float64\n",
      "y type:  int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the local folder \"data\"\n",
    "mnist = loadmat('data/mnist-original.mat')\n",
    "\n",
    "#Create the data Matrix X and the target vector y\n",
    "X = mnist[\"data\"].T.astype('float64')\n",
    "y = mnist[\"label\"][0].astype('int64')\n",
    "\n",
    "\n",
    "# Load data using Scikit-Learn\n",
    "# mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "# X = mnist[\"data\"].astype('float64')\n",
    "# y = mnist[\"target\"].astype('int64')\n",
    "\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)\n",
    "\n",
    "print(\"\\nX type: \", X.dtype)\n",
    "print(\"y type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets\n",
    "\n",
    "We use sklearn's train_test_split function to spilt the dataset into training and test subsets. The data is shuffled by default before splitting.\n",
    "\n",
    "This function splits arrays or matrices into random train and test subsets.\n",
    "\n",
    "For the reproducibility of the results, we need to use the same seed for the random number generator. The seed is set by the \"random_state\" parameter of the split function.\n",
    "\n",
    "However, in repeated experiments if we don't want to use the same train and test subsets, then we drop the \"random_state\" parameter from the funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Using Dimensionaly Reduction\n",
    "\n",
    "We can optimize the running-time of the MLP model by reducing the number of features. Our assumption is that the essence or core content of the data does not span along all dimensions. The technique for reducing the dimension of data is known as dimensionality reduction.\n",
    "\n",
    "For a gentle introduction to various dimensionality reduction technique, see the notebook \"Dimensionality Reduction\" in the Github repository.\n",
    "\n",
    "We will use the **Principle Component Analysis (PCA)** dimensionality reduction technique to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance. \n",
    "\n",
    "The goal is to see the improvement in training time due to this dimensionality reduction.\n",
    "\n",
    "Before we apply the PCA, we need to standardize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "\n",
    "PCA is influenced by scale of the data. Thus we need to scale the features of the data before applying PCA. \n",
    "\n",
    "For understanding the negative effect of not scaling the data, see the following post:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n",
    "\n",
    "Note that we fit the scaler on the training set and transform on the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA\n",
    "\n",
    "While applying PCA we can set the number of principle components by the \"n_components\" attribute. But more importantly, we can use this attribute to determine the % of variance we want to retain in the extracted features.\n",
    "\n",
    "For example, if we set it to 0.95, sklearn will choose the **minimum number of principal components** such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 920 ms, total: 14 s\n",
      "Wall time: 4.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Principle Components\n",
    "\n",
    "We can find how many components PCA chose after fitting the model by using the following attribute: n_components_\n",
    "\n",
    "We will see that 95% of the variance amounts to **315 principal components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principle Components:  330\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Principle Components: \", pca.n_components_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Mapping (Transform) to both the Training Set and the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We will conduct the following experiments.\n",
    "\n",
    "- Experiment 1: Multi-Layer Perceptron \n",
    "- Experiment 2: Multi-Layer Perceptron + PCA\n",
    "- Experiment 2: Support Vector Machine (SVC with RBF Kernel) + PCA\n",
    "- Experiment 3: Logistic Regression (Softmax Regression) + PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: MLP \n",
    "\n",
    "First we train the MLP without applying PCA on the data. So we use 784 features.\n",
    "\n",
    "See the notebook \"Perceptron-MLP-Nonlinear Data\" for a discussion on various solvers that are used by MLP and the hyperparameters.\n",
    "\n",
    "\n",
    "## Model Selection: Hyperparameter Tunining\n",
    "\n",
    "\n",
    "We use Scikit-Learnâ€™s GridSearchCV to search the combinations of hyperparameter values that provide the best performance.\n",
    "\n",
    "We did not tune all hyperparameters. Also note that the range of values for the hyperparameters should be chosen thoughtfully. We used prior knowledge based on theoretical and empirical understanding to choose the range of values.\n",
    "\n",
    "\n",
    "We tune the following hyperparameters of the GridSearchCV model.\n",
    "\n",
    "\n",
    "- hidden_layer_sizes: only one hidden layer is enough to produce optimal accuracy. Number of neurons should be raged from 100 to 200. \n",
    "\n",
    "- activation : â€˜logisticâ€™ and â€˜reluâ€™. \n",
    "\n",
    "- solver : Since the dataset is large, we did not use 'lbfgs' as it is suitable for small dataset for faster convergence. The two solvers used for tuning are: â€˜sgdâ€™ and â€˜adamâ€™. \n",
    "\n",
    "- alpha : L2 penalty (regularization term) parameter.\n",
    "\n",
    "- learning_rate : â€˜constantâ€™ and â€˜adaptiveâ€™. Note that the solver 'adam' works with 'constant' learning rate as it implements a learning schedule to decrease learning rate dynamically.\n",
    "\n",
    "- learning_rate_init : The initial learning rate used. It controls the step-size in updating the weights for 'adam' and 'sgd'. The solver 'adam' requires smaller initial learning rate (0.001), while 'sgd' requires a slightly larger value (0.1).\n",
    "\n",
    "\n",
    "For performing the hyperparameter tunining, we set the following attributes of the model to fixed values.\n",
    "\n",
    "- The \"early_stopping\" parameter is set to True to prevent overfitting (that is caused by overtraining).\n",
    "\n",
    "- If we use early stopping, then we should also set the \"n_iter_no_change\" to a suitable value. It defines the maximum number of epochs to not meet tol improvement. The default is 10.\n",
    "\n",
    "- tol : Tolerance for the optimization.\n",
    "\n",
    "\n",
    "### Hyperparameters for Faster Optimizers\n",
    "\n",
    "\n",
    "#### Optimizer: SGD\n",
    "\n",
    "- momentum : float, default 0.9\n",
    "\n",
    "        Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=â€™sgdâ€™.\n",
    "\n",
    "- nesterovs_momentum : boolean, default True\n",
    "\n",
    "        Whether to use Nesterovâ€™s momentum. Only used when solver=â€™sgdâ€™ and momentum > 0.\n",
    "        \n",
    "  \n",
    "#### Optimizer: Adam\n",
    "- beta_1 : float, optional, default 0.9\n",
    "\n",
    "        Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=â€™adamâ€™\n",
    "\n",
    "- beta_2 : float, optional, default 0.999\n",
    "\n",
    "        Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=â€™adamâ€™\n",
    "\n",
    "- epsilon : float, optional, default 1e-8\n",
    "\n",
    "        Value for numerical stability in adam. Only used when solver=â€™adamâ€™\n",
    "\n",
    "\n",
    "More info: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "For a brief discussion on the faster optimizers (momentum, adam), see the 2nd notebook on MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'hidden_layer_sizes': [(100,), (150,), (200,)], \"solver\":['sgd', 'adam'], \n",
    "              'learning_rate_init': (0.1, 0.01, 0.001), \"alpha\": (0.1, 0.01),\n",
    "              'activation': ['logistic', 'relu']}\n",
    "\n",
    "clf_mlp = MLPClassifier(early_stopping=True, n_iter_no_change=10, tol=1e-5, max_iter=500, random_state=1)\n",
    "\n",
    "clf_mlp_cv = GridSearchCV(clf_mlp, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "clf_mlp_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_mlp = clf_mlp_cv.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % clf_mlp_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_mlp)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
    "- [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "- [Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 23.8min\n",
    "- [Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 177.4min\n",
    "- [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 298.9min finished\n",
    "\n",
    "Best Score (accuracy): 0.973393\n",
    "\n",
    "Optimal Hyperparameter Values:  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (200,), 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
    "\n",
    "\n",
    "Wall time: 5h 31s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.32298691\n",
      "Validation score: 0.949464\n",
      "Iteration 2, loss = 0.13119739\n",
      "Validation score: 0.962321\n",
      "Iteration 3, loss = 0.09264147\n",
      "Validation score: 0.965714\n",
      "Iteration 4, loss = 0.06617721\n",
      "Validation score: 0.970357\n",
      "Iteration 5, loss = 0.05117022\n",
      "Validation score: 0.967321\n",
      "Iteration 6, loss = 0.04344897\n",
      "Validation score: 0.971607\n",
      "Iteration 7, loss = 0.03361663\n",
      "Validation score: 0.972500\n",
      "Iteration 8, loss = 0.02845721\n",
      "Validation score: 0.972321\n",
      "Iteration 9, loss = 0.02450681\n",
      "Validation score: 0.972857\n",
      "Iteration 10, loss = 0.02429279\n",
      "Validation score: 0.974464\n",
      "Iteration 11, loss = 0.02010919\n",
      "Validation score: 0.974643\n",
      "Iteration 12, loss = 0.01814253\n",
      "Validation score: 0.976607\n",
      "Iteration 13, loss = 0.01674399\n",
      "Validation score: 0.974464\n",
      "Iteration 14, loss = 0.01568636\n",
      "Validation score: 0.976429\n",
      "Iteration 15, loss = 0.01513316\n",
      "Validation score: 0.975179\n",
      "Iteration 16, loss = 0.01716468\n",
      "Validation score: 0.964643\n",
      "Iteration 17, loss = 0.06938609\n",
      "Validation score: 0.963036\n",
      "Iteration 18, loss = 0.05584488\n",
      "Validation score: 0.972857\n",
      "Iteration 19, loss = 0.03304993\n",
      "Validation score: 0.974107\n",
      "Iteration 20, loss = 0.02083562\n",
      "Validation score: 0.975714\n",
      "Iteration 21, loss = 0.01819509\n",
      "Validation score: 0.976250\n",
      "Iteration 22, loss = 0.01686599\n",
      "Validation score: 0.976786\n",
      "Iteration 23, loss = 0.01614675\n",
      "Validation score: 0.976786\n",
      "Iteration 24, loss = 0.01553364\n",
      "Validation score: 0.976250\n",
      "Iteration 25, loss = 0.01496594\n",
      "Validation score: 0.976607\n",
      "Iteration 26, loss = 0.01440323\n",
      "Validation score: 0.976786\n",
      "Iteration 27, loss = 0.01382349\n",
      "Validation score: 0.976071\n",
      "Iteration 28, loss = 0.01337950\n",
      "Validation score: 0.976429\n",
      "Iteration 29, loss = 0.01329879\n",
      "Validation score: 0.975714\n",
      "Iteration 30, loss = 0.04046345\n",
      "Validation score: 0.962143\n",
      "Iteration 31, loss = 0.06578276\n",
      "Validation score: 0.969107\n",
      "Iteration 32, loss = 0.04021290\n",
      "Validation score: 0.974286\n",
      "Iteration 33, loss = 0.02237942\n",
      "Validation score: 0.975000\n",
      "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The MLP takes 47.4s.\n",
      "No. of Iterations: 33\n",
      "\n",
      "Training Accuracy:  0.9976785714285714\n",
      "CPU times: user 2min 52s, sys: 9.1 s, total: 3min 1s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(200,), max_iter=200, alpha=0.01,\n",
    "                    solver='adam', verbose=True, tol=1e-5, random_state=1, \n",
    "                    learning_rate='constant', learning_rate_init=0.001, activation='relu',\n",
    "                    early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "duration_mlp = t1 - t0\n",
    "print(\"The MLP takes {:.1f}s.\".format(duration_mlp))\n",
    "\n",
    "print(\"No. of Iterations:\", mlp_clf.n_iter_ )\n",
    "\n",
    "y_train_predicted = mlp_clf.predict(X_train)\n",
    "\n",
    "train_accuracy_mlp = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Evaluate MLP  on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9766428571428571\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1296    0    1    0    1    1    9    2    2    0]\n",
      " [   0 1587    8    2    1    0    1    3    2    0]\n",
      " [   1    5 1318    8    5    0    2    2    5    2]\n",
      " [   0    3   11 1387    0   11    0    6    6    3]\n",
      " [   1    2    6    0 1321    1    6    5    2   18]\n",
      " [   1    0    3   10    0 1241    7    2   12    4]\n",
      " [   3    2    2    0    2    4 1383    0    1    0]\n",
      " [   2    3    7    2    3    1    0 1428    2   13]\n",
      " [   0    3    4    9    2    3    4    6 1355    4]\n",
      " [   4    3    3    7   17    3    1   16    8 1357]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1312\n",
      "           1       0.99      0.99      0.99      1604\n",
      "           2       0.97      0.98      0.97      1348\n",
      "           3       0.97      0.97      0.97      1427\n",
      "           4       0.98      0.97      0.97      1362\n",
      "           5       0.98      0.97      0.98      1280\n",
      "           6       0.98      0.99      0.98      1397\n",
      "           7       0.97      0.98      0.97      1461\n",
      "           8       0.97      0.97      0.97      1390\n",
      "           9       0.97      0.96      0.96      1419\n",
      "\n",
      "    accuracy                           0.98     14000\n",
      "   macro avg       0.98      0.98      0.98     14000\n",
      "weighted avg       0.98      0.98      0.98     14000\n",
      "\n",
      "CPU times: user 465 ms, sys: 26.1 ms, total: 491 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_predicted = mlp_clf.predict(X_test)\n",
    "\n",
    "accuracy_score_test_mlp = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test_mlp)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expriment 2: MLP + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49712015\n",
      "Validation score: 0.935357\n",
      "Iteration 2, loss = 0.19141485\n",
      "Validation score: 0.948750\n",
      "Iteration 3, loss = 0.14316349\n",
      "Validation score: 0.957679\n",
      "Iteration 4, loss = 0.11065116\n",
      "Validation score: 0.964286\n",
      "Iteration 5, loss = 0.08852195\n",
      "Validation score: 0.967679\n",
      "Iteration 6, loss = 0.07122619\n",
      "Validation score: 0.967857\n",
      "Iteration 7, loss = 0.06032630\n",
      "Validation score: 0.969821\n",
      "Iteration 8, loss = 0.05056308\n",
      "Validation score: 0.973214\n",
      "Iteration 9, loss = 0.04944693\n",
      "Validation score: 0.970893\n",
      "Iteration 10, loss = 0.04480907\n",
      "Validation score: 0.972500\n",
      "Iteration 11, loss = 0.03962612\n",
      "Validation score: 0.974286\n",
      "Iteration 12, loss = 0.04125004\n",
      "Validation score: 0.973750\n",
      "Iteration 13, loss = 0.03956243\n",
      "Validation score: 0.972500\n",
      "Iteration 14, loss = 0.03339130\n",
      "Validation score: 0.974286\n",
      "Iteration 15, loss = 0.02977469\n",
      "Validation score: 0.974107\n",
      "Iteration 16, loss = 0.02981106\n",
      "Validation score: 0.975000\n",
      "Iteration 17, loss = 0.04301179\n",
      "Validation score: 0.973214\n",
      "Iteration 18, loss = 0.03363088\n",
      "Validation score: 0.971429\n",
      "Iteration 19, loss = 0.03169536\n",
      "Validation score: 0.974107\n",
      "Iteration 20, loss = 0.02952275\n",
      "Validation score: 0.973393\n",
      "Iteration 21, loss = 0.02781989\n",
      "Validation score: 0.973393\n",
      "Iteration 22, loss = 0.02084676\n",
      "Validation score: 0.975357\n",
      "Iteration 23, loss = 0.01859136\n",
      "Validation score: 0.976607\n",
      "Iteration 24, loss = 0.01781503\n",
      "Validation score: 0.976429\n",
      "Iteration 25, loss = 0.01821324\n",
      "Validation score: 0.975357\n",
      "Iteration 26, loss = 0.02116357\n",
      "Validation score: 0.974464\n",
      "Iteration 27, loss = 0.02471338\n",
      "Validation score: 0.975357\n",
      "Iteration 28, loss = 0.02460259\n",
      "Validation score: 0.976250\n",
      "Iteration 29, loss = 0.02596891\n",
      "Validation score: 0.975179\n",
      "Iteration 30, loss = 0.02174741\n",
      "Validation score: 0.975357\n",
      "Iteration 31, loss = 0.02002380\n",
      "Validation score: 0.975357\n",
      "Iteration 32, loss = 0.01751132\n",
      "Validation score: 0.976250\n",
      "Iteration 33, loss = 0.01776240\n",
      "Validation score: 0.975536\n",
      "Iteration 34, loss = 0.01679814\n",
      "Validation score: 0.975536\n",
      "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The PCA+MLP takes 25.7s.\n",
      "No. of Iterations: 34\n",
      "\n",
      "Training Accuracy:  0.9974821428571429\n",
      "CPU times: user 1min 39s, sys: 3.42 s, total: 1min 42s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "mlp_clf_pca = MLPClassifier(hidden_layer_sizes=(200,), max_iter=200, alpha=0.01,\n",
    "                    solver='adam', verbose=True, tol=1e-5, random_state=1, \n",
    "                    learning_rate='constant', learning_rate_init=0.001, activation='relu',\n",
    "                    early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "\n",
    "mlp_clf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "duration_mlp_pca = t1 - t0\n",
    "print(\"The PCA+MLP takes {:.1f}s.\".format(duration_mlp_pca))\n",
    "\n",
    "print(\"No. of Iterations:\", mlp_clf_pca.n_iter_ )\n",
    "\n",
    "y_train_predicted = mlp_clf_pca.predict(X_train_pca)\n",
    "\n",
    "train_accuracy_mlp = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Evaluate MLP + PCA on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9735714285714285\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1297    0    2    0    0    1    5    4    3    0]\n",
      " [   0 1584   11    1    2    0    0    2    3    1]\n",
      " [   3    5 1312    5    3    1    5    8    4    2]\n",
      " [   1    3   11 1383    0   16    0    4    5    4]\n",
      " [   2    1    7    0 1320    2    6    3    2   19]\n",
      " [   1    1    3   12    1 1233    9    4   12    4]\n",
      " [   4    2    3    0    4    6 1375    2    1    0]\n",
      " [   1    2    9    3    5    2    1 1421    4   13]\n",
      " [   1    6    6    8    3    7    4    4 1345    6]\n",
      " [   3    2    3    8   17    5    0   13    8 1360]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1312\n",
      "           1       0.99      0.99      0.99      1604\n",
      "           2       0.96      0.97      0.97      1348\n",
      "           3       0.97      0.97      0.97      1427\n",
      "           4       0.97      0.97      0.97      1362\n",
      "           5       0.97      0.96      0.97      1280\n",
      "           6       0.98      0.98      0.98      1397\n",
      "           7       0.97      0.97      0.97      1461\n",
      "           8       0.97      0.97      0.97      1390\n",
      "           9       0.97      0.96      0.96      1419\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "CPU times: user 354 ms, sys: 25.1 ms, total: 379 ms\n",
      "Wall time: 95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_predicted = mlp_clf_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy_score_test_mlp_pca = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test_mlp_pca)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: SVC (RBF Kernel) + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PCA+SVM takes 193.4s.\n",
      "CPU times: user 3min 12s, sys: 591 ms, total: 3min 13s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "svm_clf_pca = SVC(C=1, gamma=0.001)\n",
    "svm_clf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "duration_svm_pca = t1 - t0\n",
    "print(\"The PCA+SVM takes {:.1f}s.\".format(duration_svm_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Evaluate SVC (RBF Kernel) + PCA on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9652857142857143\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1297    0    1    0    0    1    8    2    3    0]\n",
      " [   0 1574   11    2    1    0    0    8    4    4]\n",
      " [   3    4 1301   11    6    1    7    8    5    2]\n",
      " [   0    4   15 1366    2    9    0   15   11    5]\n",
      " [   3    2    8    1 1312    1    5    7    3   20]\n",
      " [   2    1    4   20    3 1217   13    6   10    4]\n",
      " [   5    1    3    0    2    7 1369    7    3    0]\n",
      " [   5    3   13    3    7    1    0 1411    1   17]\n",
      " [   0   11    8   13    4    8    6    4 1334    2]\n",
      " [   3    5    6   16   23    5    1   24    3 1333]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1312\n",
      "           1       0.98      0.98      0.98      1604\n",
      "           2       0.95      0.97      0.96      1348\n",
      "           3       0.95      0.96      0.96      1427\n",
      "           4       0.96      0.96      0.96      1362\n",
      "           5       0.97      0.95      0.96      1280\n",
      "           6       0.97      0.98      0.98      1397\n",
      "           7       0.95      0.97      0.96      1461\n",
      "           8       0.97      0.96      0.96      1390\n",
      "           9       0.96      0.94      0.95      1419\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "CPU times: user 1min 9s, sys: 193 ms, total: 1min 9s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_test_predicted = svm_clf_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy_score_test_svm = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test_svm)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Logistic Regression (Softmax Regression) + PCA\n",
    "\n",
    "We use the best performing solver (i.e., lbfgs) from previous notebook to train the logistic regression model on the PCA transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PCA + Logistic Regression takes 4.0s.\n",
      "CPU times: user 15.3 s, sys: 344 ms, total: 15.6 s\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "softmax_reg_pca = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "duration_lr_pca = t1 - t0\n",
    "print(\"The PCA + Logistic Regression takes {:.1f}s.\".format(duration_lr_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Evaluate Softmax Regression + PCA on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Test Accuracy:  0.9220714285714285\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1270    0    4    3    5    9   11    3    7    0]\n",
      " [   0 1557   10    7    2    4    0    2   16    6]\n",
      " [  10   22 1211   24   15    2   22   13   24    5]\n",
      " [   3   14   27 1283    1   44    3    8   33   11]\n",
      " [   4    8    9    2 1267    3   10    4    9   46]\n",
      " [   9    9   13   43   12 1111   21    8   40   14]\n",
      " [   9    4    8    1    9   15 1342    2    5    2]\n",
      " [   5    3   23    4   14    1    1 1363    1   46]\n",
      " [   6   28   12   38    7   32   10    3 1239   15]\n",
      " [   8    5    4   25   46   11    1   43   10 1266]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      1312\n",
      "           1       0.94      0.97      0.96      1604\n",
      "           2       0.92      0.90      0.91      1348\n",
      "           3       0.90      0.90      0.90      1427\n",
      "           4       0.92      0.93      0.92      1362\n",
      "           5       0.90      0.87      0.88      1280\n",
      "           6       0.94      0.96      0.95      1397\n",
      "           7       0.94      0.93      0.94      1461\n",
      "           8       0.90      0.89      0.89      1390\n",
      "           9       0.90      0.89      0.89      1419\n",
      "\n",
      "    accuracy                           0.92     14000\n",
      "   macro avg       0.92      0.92      0.92     14000\n",
      "weighted avg       0.92      0.92      0.92     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_pca.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_pca.predict(X_test_pca)\n",
    "\n",
    "\n",
    "accuracy_score_test_lr = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test_lr)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results from 3 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.976643</td>\n",
       "      <td>47.365682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP + PCA</td>\n",
       "      <td>0.973571</td>\n",
       "      <td>25.742746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM(RBF) + PCA</td>\n",
       "      <td>0.965286</td>\n",
       "      <td>193.355095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Softmax + PCA</td>\n",
       "      <td>0.922071</td>\n",
       "      <td>4.012608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier  Test Accuracy  Running-Time\n",
       "0             MLP       0.976643     47.365682\n",
       "1       MLP + PCA       0.973571     25.742746\n",
       "2  SVM(RBF) + PCA       0.965286    193.355095\n",
       "3   Softmax + PCA       0.922071      4.012608"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"MLP\", accuracy_score_test_mlp, duration_mlp], \n",
    "        [\"MLP + PCA\", accuracy_score_test_mlp_pca, duration_mlp_pca], \n",
    "        [\"SVM(RBF) + PCA\", accuracy_score_test_svm, duration_svm_pca],\n",
    "        [\"Softmax + PCA\", accuracy_score_test_lr, duration_lr_pca]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Classifier\", \"Test Accuracy\", \"Running-Time\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Understanding\n",
    "\n",
    "We have done 4 experiments using MLP (without PCA), MLP (with PCA), Kernel SVM (with PCA) and Logistic Regression (with PCA) classifiers.\n",
    "\n",
    "We make following observations.\n",
    "- The MLP outperforms other two classifiers.\n",
    "- MLP with PCA performs slightly less than MLP without PCA, but it is faster.\n",
    "- Understandably logistic regression performed poorly due to the non-linear nature of the data. However, it is faster.\n",
    "- MLP (with PCA) is faster than the Kernel SVM. Because the dual SVM optimization complexity is $O(N^2d)$.\n",
    "\n",
    "### Thus, for large non-linear data set (e.g., image classification) MLP performs better than the RBF kernel based SVM."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
